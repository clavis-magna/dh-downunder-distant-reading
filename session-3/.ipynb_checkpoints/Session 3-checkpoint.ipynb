{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DH Downunder: Distant Reading\n",
    "\n",
    "## Notebook 3: Stylistics, or Computer Magic\n",
    "\n",
    "One of the most interesting applications of distant reading techniques is authorship attribution. In this third and final notebook for our Distant Reading course, we will consider how text analysis can be used to work out the true identity of a text's author.\n",
    "\n",
    "Authorship attribution will also give us a chance to introduce several concepts from data science, such as vectors, distance metrics and clustering. These concepts are useful in a number of areas beyond authorship attribution.\n",
    "\n",
    "Execute the cell below to load all the packages required for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for doing maths\n",
    "import nltk # for natural language processing functions\n",
    "import matplotlib # for displaying graphs\n",
    "import random # some useful randomisation functions\n",
    "import scipy\n",
    "from import_corpus import import_corpus\n",
    "\n",
    "# So graphs render properly\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load and Inspect our Corpus\n",
    "\n",
    "For this notebook, I have pre-prepared a corpus for you. Execute the cell below to import all the novels into your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyllard's Weird, by Mary Braddon successfully imported.\n",
      "Castle Rackrent, by Maria Edgeworth successfully imported.\n",
      "The Hungry Stones, and Other Stories, by Rabindranath Tagore successfully imported.\n",
      "The Talisman, by Sir Walter Scott successfully imported.\n",
      "Ivanhoe, by Sir Walter Scott successfully imported.\n",
      "Belinda, by Maria Edgeworth successfully imported.\n",
      "The Absentee, by Maria Edgeworth successfully imported.\n",
      "The Blithdale Romance, by Nathaniel Hawthorne successfully imported.\n",
      "Old Mortality, by Sir Walter Scott successfully imported.\n",
      "The Crimson Cryptogram, by Fergus Hume successfully imported.\n",
      "The Heart of Mid-Lothian, by Sir Walter Scott successfully imported.\n",
      "Lady Audley's Secret, by Mary Braddon successfully imported.\n",
      "Waverley; or, 'tis Sixty Years Since, by ??? successfully imported.\n",
      "The Disappearing Eye, by Fergus Hume successfully imported.\n",
      "The Bride of Lammermoor, by Sir Walter Scott successfully imported.\n",
      "The Lost Parchment, by Fergus Hume successfully imported.\n",
      "The Mystery of the Hansom Cab, by Fergus Hume successfully imported.\n",
      "An Australian Girl, by Catherine Martin successfully imported.\n",
      "Rob Roy, by Sir Walter Scott successfully imported.\n",
      "The Scarlet Letter, by Nathaniel Hawthorne successfully imported.\n",
      "Mashi, and Other Stories, by Rabindranath Tagore successfully imported.\n",
      "The House of the Seven Gables, by Nathaniel Hawthorne successfully imported.\n",
      "\n",
      "22 novels imported, by 8 unique authors.\n"
     ]
    }
   ],
   "source": [
    "novels = import_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one mystery novel in our corpus! In 1814, the novel *Waverley, or 'tis Sixty Years Since* was published anonymously. In this session we are going to use statistical analysis to find out who ??? really was.\n",
    "\n",
    "The whole corpus, `novels` is a single `dict`. Execute the cell below to see how to access information from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The keys for the novels dict are:\n",
      "  -  wyllards_weird\n",
      "  -  rackrent\n",
      "  -  hungry_stones\n",
      "  -  talisman\n",
      "  -  ivanhoe\n",
      "  -  belinda\n",
      "  -  absentee\n",
      "  -  blithedale_romance\n",
      "  -  old_mortality\n",
      "  -  crimson_cryptogram\n",
      "  -  mid_lothian\n",
      "  -  lady_audley\n",
      "  -  waverley\n",
      "  -  disappearing_eye\n",
      "  -  lammermoor\n",
      "  -  lost_parchment\n",
      "  -  hansom_cab\n",
      "  -  australian_girl\n",
      "  -  rob_roy\n",
      "  -  scarlet_letter\n",
      "  -  mashi\n",
      "  -  seven_gables\n",
      "\n",
      "\n",
      "Some examples of how to find information:\n",
      "\n",
      "The title and author of Castle Rackrent:\n",
      "novels[\"rackrent\"][\"title\"] = Castle Rackrent\n",
      "novels[\"rackrent\"][\"author\"] = Maria Edgeworth\n",
      "\n",
      "Tokens 1000-1009 of The Hungry Stones:\n",
      "novels[\"hungry_stones\"][\"tokens\"][1000:1010] = ['laden', 'with', 'an', 'oppressive', 'scent', 'from', 'the', 'spicy', 'shrubs', 'growing']\n",
      "\n",
      "For each novel, the following information is available:\n",
      "  -  title\n",
      "  -  author\n",
      "  -  header\n",
      "  -  licence\n",
      "  -  body\n",
      "  -  tokens\n"
     ]
    }
   ],
   "source": [
    "corpus_keys = '\\n  -  '.join(list(novels))\n",
    "novel_keys = '\\n  -  '.join(list(novels['waverley']))\n",
    "print(f'The keys for the novels dict are:\\n  -  {corpus_keys}\\n\\n')\n",
    "\n",
    "print(f'Some examples of how to find information:\\n')\n",
    "print(f'The title and author of Castle Rackrent:')\n",
    "print(f'novels[\"rackrent\"][\"title\"] = {novels[\"rackrent\"][\"title\"]}')\n",
    "print(f'novels[\"rackrent\"][\"author\"] = {novels[\"rackrent\"][\"author\"]}')\n",
    "print('\\nTokens 1000-1009 of The Hungry Stones:')\n",
    "print(f'novels[\"hungry_stones\"][\"tokens\"][1000:1010] = {novels[\"hungry_stones\"][\"tokens\"][1000:1010]}')\n",
    "print(f'\\nFor each novel, the following information is available:\\n  -  {novel_keys}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the body text and tokens have been put in lowercase, as discussed in our previous sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Calculate word frequencies for each novel\n",
    "\n",
    "One of the most interesting findings of distant reading over the last few decades has been that each person has a detectable stylistic 'signature'. It seems that when we write, each of us uses the extremely common words, 'the', 'a', 'and', 'for', 'but' in a particular ratio which is more-or-less unique. In the most famous paper in the field of stylometry, John Burrows showed that we can use this fact to help identify the authors of disputed texts:\n",
    "\n",
    "* John Burrows, [‘Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship](https://doi.org/10.1093/llc/17.3.267), *Literary and Linguistic Computing* (2002) 17: 267-87.\n",
    "\n",
    "There are many other measurements of stylistic similarity, and Burrows himself argues that 'Delta' is not sufficient on its own, but it is a good place to start the study of authorship attribution. In this notebook, you will learn exactly how to calculate Burrows' Delta and use it to find out who wrote *Waverley*.\n",
    "\n",
    "The first step is to calculate the word frequencies for all the words in each novel. How often does each writer use each of the words in their vocabulary?\n",
    "\n",
    "We can use Python's `set()` object and the `.count()` list method to do this. A `set()` object is a collection of *unique* values. Execute the cell below to see how a `set()` can be used to get the vocabulary of a text from a list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the Set you just created from a_list_of_tokens: {'Skywalker', 'my', 'hello', 'Luke', 'name', 'Han', 'is', 'Solo'}\n"
     ]
    }
   ],
   "source": [
    "a_list_of_tokens = ['hello','my','name','is','Han','Solo','hello','my','name','is','Luke','Skywalker']\n",
    "\n",
    "a_set_of_tokens = set(a_list_of_tokens) # Conver the list into a set.\n",
    "\n",
    "print(f'Here is the Set you just created from a_list_of_tokens: {a_set_of_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new `set` now contains exactly one copy of each word that appears in the original `list`. Notice for example that there is only one 'hello', one 'name' and one 'is'. Sets do not preserve the order of the original values, but this is immaterial for our purposes.\n",
    "\n",
    "Notice that the new `set` is enclosed in curly braces `{}`. This is an unfortunate abiguity in Python, since `dicts` are also enclosed in curly braces `{}`. If you are ever unsure whether you are dealing with a `set` or a `dict`, you can use the `type` function to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a_set_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we can quickly and easily find out what all the words are in a given novel. Now we need to work out how to count all those words. The `.count()` method gives us a simple way to do this. Execute the cell below to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the opening stanza of this lovely children's poem, \"James\" appears 4 times, and \"of\" appears 2 times.\n"
     ]
    }
   ],
   "source": [
    "another_list_of_tokens = ['James','James','Morrison','Morrison','Wetherby','George','Dupree',\n",
    "                         'took','great','care','of','his','mother',\n",
    "                         'though','he','was','only','three',\n",
    "                         'James','James','Morrison','Morrison','said','to','his','mother','said','he',\n",
    "                         'you','must','never','go','down','to','the','end','of','the','town',\n",
    "                         'if','you','don\\'t','go','down','with','me']\n",
    "\n",
    "n_james = another_list_of_tokens.count('James')\n",
    "n_of = another_list_of_tokens.count('of')\n",
    "\n",
    "print(f'In the opening stanza of this lovely children\\'s poem, \"James\" appears {n_james} times, and \"of\" appears {n_of} times.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one other important point is that texts can vary considerably in length. Two books may both use the word 'the' 2000 times, but if book A is 10,000 words long, and book B is 10,000,000 words long, then obviously these 2000 'the's would mean very different things!\n",
    "\n",
    "The usual way to express word frequencies is therefore 'occurrences per 1000 words'. The formula for calcuating this is $$\\frac{n_x}{t} \\times 1000 $$ where $n_x$ is the number of times word $x$ appears in the text and $t$ is the total words in the text.\n",
    "\n",
    "### Exercise 2.1: Compute the relative word frequency of 'scotland'\n",
    "\n",
    "Complete the code in the cell below to calculate the frequency per 1000 words of the word 'scotland' in *Waverley* (remember all words have been put into lower case). You can use `len()`, `.count('word')`, `*` and `/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"scotland\" appears 0.509 times per 1000 words in Waverley.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "waverley_tokens =          # Get Waverley's tokens from the 'novels' dict\n",
    "scotland_count =           # Count the number of times 'scotland' is used\n",
    "total_words =              # Count the total number of words in the novel\n",
    "rel_freq_scotland =        # Apply the formula above\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "print(f'The word \"scotland\" appears {rel_freq_scotland:.3f} times per 1000 words in Waverley.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:** The word \"scotland\" appears 0.509 times per 1000 words in Waverley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Find the 20 most common words in *Waverley*\n",
    "\n",
    "Now let's find the 20 most common words in the novel. You can use `set()` on the list of *Waverley*'s tokens to get the vocabulary of the novel, then you can use a `for` loop to look for each word in the novel and apply the formula above. I have provided the code that will fetch the top twenty words when you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "waverley_tokens =             # Get Waverlery's tokens from the 'novels' dict\n",
    "total_words =                 # Use len() to get the total words\n",
    "waverley_vocab =              # Use set() to get all the unique words\n",
    "\n",
    "results = {}                  # Initialise results dict (done for you)\n",
    "\n",
    "for word in waverley_vocab:   # Loop over the set of unique words (done for you)\n",
    "    \n",
    "    word_count =              # How many times does this word appear in the novel?\n",
    "    \n",
    "    per_1000 =                # How many times does this word appear per 1000 words in the novel (use formula)\n",
    "    \n",
    "    results[word] = per_1000  # Add the result to the results dict (done for you)\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "# Sort the results:\n",
    "top_20 = [(k, results[k]) for k in sorted(results, key=results.get, reverse=True)][0:20]\n",
    "\n",
    "# Display them:\n",
    "top_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make any guess about the author based on this information?\n",
    "\n",
    "Obviously this method works, but it is very slow. Many programming libraries contain fast, useful functions for just this sort of task. One useful one is `FreqDist()`, which is provided by the Natural Langauge Tooklit. It has a very useful feature, the `.most_common()` method, which you can use to extract the most common words in a text. Execute the cell below to see how this function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 14164),\n",
       " ('of', 8856),\n",
       " ('and', 6718),\n",
       " ('to', 6116),\n",
       " ('a', 4788),\n",
       " ('in', 3706),\n",
       " ('his', 3458),\n",
       " ('he', 2715),\n",
       " ('was', 2423),\n",
       " ('that', 2191),\n",
       " ('with', 2055),\n",
       " ('which', 1924),\n",
       " ('i', 1879),\n",
       " ('as', 1850),\n",
       " ('it', 1598),\n",
       " ('had', 1579),\n",
       " ('for', 1497),\n",
       " ('by', 1379),\n",
       " ('but', 1200),\n",
       " ('at', 1196)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "waverley_freqs = FreqDist(novels['waverley']['tokens'])\n",
    "\n",
    "waverley_freqs.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately `FreqDist()` gives you raw frequencies. A more advanced function is the `CountVectorizer()` function from the scikit-learn package. This function takes a `list` of strings, and outputs a \"Document-Term Matrix\" (DTM). A DTM is a giant table, where each row represents a novel from your corpus, and each column is a particular word:\n",
    "\n",
    "|Novel|aaron|ab|aback|abacus|abaddon|abana|abandon|...|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|wyllards_weird|0|0|0|0|0|0|5|...|\n",
    "|rackrent|0|0|0|0|0|0|0|...|\n",
    "|hungry_stones|0|0|1|0|0|0|1|...|\n",
    "|talisman|0|0|0|1|0|0|3|...|\n",
    "|ivanhoe|2|0|0|2|0|1|6|...|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of the below matrix represent: aaron, ab, aback, abacus, abaddon, abana, abandon\n",
      "\n",
      "The rows represent:\n",
      "wyllards_weird\n",
      "rackrent\n",
      "hungry_stones\n",
      "talisman\n",
      "ivanhoe\n",
      "\n",
      "[[0 0 0 0 0 0 5]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 1]\n",
      " [0 0 0 1 0 0 3]\n",
      " [2 0 0 2 0 1 6]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer requires us to provide a list, where each item of the list is a text represented\n",
    "# by a single string. So first we will extract all the texts into text_list, and keep track of which\n",
    "# text is which using novel_list.\n",
    "novel_list = []\n",
    "text_list = []\n",
    "for name, novel in novels.items():\n",
    "    novel_list.append(name)\n",
    "    text_list.append(novel['body'])\n",
    "    \n",
    "# Initialise the 'vectorizer', the object we can use to convert the novels into a DTM:\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Apply the vectorizer to the novels. The .fit_transform method performs the two steps we performed above:\n",
    "# fit = find the vocabulary of all the texts\n",
    "# transform = count the instances of each vocabulary word in each text\n",
    "DTM = vectorizer.fit_transform(text_list)\n",
    "\n",
    "print(f'The columns of the below matrix represent: {\", \".join(vectorizer.get_feature_names()[63:70])}\\n')\n",
    "row_names = \"\\n\".join(novel_list[0:5])\n",
    "print(f'The rows represent:\\n{row_names}\\n')\n",
    "print(DTM.toarray()[0:5,63:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, these are also raw frequencies, and we would like to have the relative frequencies. This is very easy to compute using the built-in `.sum()` method. The object we have just created, `DTM`, is a [csr sparse matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html), which has many useful built-in methods besides `.sum()`. \n",
    "\n",
    "Since each *row* represents one novel, if we add up all the numbers in a single *row*, we will get the total words for that novel.\n",
    "\n",
    "### Exercise 2.3: Calculate the relative frequencies.\n",
    "\n",
    "In the cell below, use the `.sum()` method to calculate the sum for each row. Then \n",
    "\n",
    "**NB:** You have to use the extra parameter `axis = 1` when you call the `.sum()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of the below matrix represent: aaron, ab, aback, abacus, abaddon, abana\n",
      "\n",
      "The rows represent:\n",
      "wyllards_weird\n",
      "rackrent\n",
      "hungry_stones\n",
      "talisman\n",
      "ivanhoe\n",
      "\n",
      "[[0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.02137529 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.00790883 0.         0.        ]\n",
      " [0.01058985 0.         0.         0.01058985 0.         0.00529493]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "row_sums =        # Use .sum(axis = 1) to find the row sums of the DTM\n",
    "DTM =             # Divide the whole DTM by row_sums and then multiply the DTM by 1000 to get the relative frequencies\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "DTM = np.array(DTM) # Type conversion to deal with quirk in software\n",
    "\n",
    "print(f'The columns of the below matrix represent: {\", \".join(vectorizer.get_feature_names()[63:69])}\\n')\n",
    "row_names = \"\\n\".join(novel_list[0:5])\n",
    "print(f'The rows represent:\\n{row_names}\\n')\n",
    "print(DTM[0:5,63:69])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Due to a quirk in the software, `DTM` has now been converted into a [numpy array](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html). This is a very similar data type to a csr sparse matrix, so don't worry too much about it. The only difference is that you will see the `keepdims = True` argument popping up now, which is not necessary when you are working with a sparse matrix.\n",
    "\n",
    "Since we have now divided each row by the total words, and multiplied by 1000, each row now adds up to 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.],\n",
       "       [1000.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTM.sum(axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to find the most common words, we can use `.sum(axis = 0)` to sum over the columns and find the words that are most frequent. This is a slightly complicated step, so I have written the code for you. It finds the top 20 words. If you want to change the number of words it finds, simply alter the `top_n` variable at the top of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 58.73744312645612),\n",
       " ('of', 32.85473748253985),\n",
       " ('and', 31.990543031657804),\n",
       " ('to', 29.546003263751686),\n",
       " ('in', 16.94207333177555),\n",
       " ('that', 13.214060937614386),\n",
       " ('he', 12.691499197428486),\n",
       " ('was', 12.556211982903273),\n",
       " ('his', 12.014947873994034),\n",
       " ('it', 10.599612892020495),\n",
       " ('you', 9.825144620133825),\n",
       " ('her', 9.48036223768839),\n",
       " ('with', 9.39666135840758),\n",
       " ('as', 9.313692730118921),\n",
       " ('had', 8.116062127092208),\n",
       " ('for', 7.904788635883245),\n",
       " ('she', 7.162326034085504),\n",
       " ('my', 7.023160500383226),\n",
       " ('but', 6.704019060707225),\n",
       " ('not', 6.615259064156846)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words do we want?\n",
    "top_n = 20\n",
    "\n",
    "# Sum over the columns. Each column is 1 word, so if we add a column up we find out how many times\n",
    "# that word was used in our corpus.\n",
    "col_sums = DTM.sum(axis = 0)\n",
    "\n",
    "# Which columns have the highest frequency?\n",
    "# NB: np.sort() sorts into ascending order, so the big numbers are at the end.\n",
    "columns_sorted = np.argsort(col_sums)\n",
    "\n",
    "# Get the top n words:\n",
    "top_list = []\n",
    "for i in range(1, top_n + 1):\n",
    "    num = -i\n",
    "    idx = columns_sorted[num]\n",
    "    word = vectorizer.get_feature_names()[idx]\n",
    "    corpus_freq = col_sums[idx] / 22 # to get the frequency per 1000 words, divide by the number of texts in the corpus\n",
    "    top_list.append((word, corpus_freq))\n",
    "\n",
    "top_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Calculate \"Burrows' Delta\" for all the novels\n",
    "\n",
    "Now we have all the tools we need to calculate Burrows' Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ir walter wrote i felt that something might be attempted for my own country of the same kind as that which miss edgeworth so fortunately achieved for ireland in the memoirs of miss edgeworth there is a pretty account of her sudden burst of feeling when this passage so unexpected and so deeply felt by her was read out by one of her sisters at a time when maria lay weak and recovering from illness in edgeworthstown our host took us that day among other pleasant things for a marvellous and delightful flight on a jaunting car to see something of the country we sped through storms and sunshine by open moors and fields and then by villages and little churches by farms where the pigs were standing at the doors to be fed by pretty trim cottages the lights came and went as the mist lifted we could see the exquisite colours the green the dazzling sweet lights on the meadows playing upon the meadow sweet and elder bushes at last we came to the lovely glades of carriglass it seemed to me that we had reached an enchanted forest amid this green sweet tangle of ivy of flowering summer trees of immemorial oaks and sycamores a squirrel was darting up the branches of a beautiful spreading beech tree a whole army of rabbits were flashing with silver tails into the brushwood swallows blackbirds peacock butterflies dragonflies on the wing a mighty sylvan life was roaming in this lovely orderly wilderness the great irish kitchen garden belonging to the house with its seven miles of wall was also not unlike a part of a fairy tale its owner mr lefroy told me that miss edgeworth had been constantly there she was a great friend of judge lefroy as a boy he remembered her driving up to the house and running up through the great drawing room doors to greet the judge miss edgeworth certainly lived in a fair surrounding and with sophia western must have gone along the way of life heralded by sweetest things by the song of birds by the gold radiance of the buttercups by the varied shadows of those'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novels['rackrent']['body'][10000:12000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Who wrote *Waverley*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels = {\n",
    "    'foo':{\n",
    "        'title':'foo',\n",
    "        'author':'bar'\n",
    "    },\n",
    "    'bar':{\n",
    "        'title':'bar strikes back',\n",
    "        'author': 'Emperor Barbatine'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bar', 'Emperor Barbatine']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[info['author'] for novel,info in novels.items()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
